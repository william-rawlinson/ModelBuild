#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu Oct 24 10:57:27 2024

@author: will.rawlinson
"""

import base64
import os
import time
from typing import Dict, List, Optional, Tuple, Union
from dotenv import load_dotenv
import boto3
import logging
from backend.src.core.llm.llm_log import llm_log
from  backend.src.core.llm.llm_stats import LLMStats
import json
from botocore.config import Config

logger = logging.getLogger(__name__)

load_dotenv()
llm = os.getenv("llm")

llm_stats = LLMStats()

##### claude models
# claude-opus-4-20250514: $15/M input, $75/M output
# claude-sonnet-4-20250514: $3/M input, $15/M output
# claude-3-7-sonnet-latest: $3/M input, $15/M output
# claude-3-5-sonnet-20241022: $3/M input, $15/M output

##### openai models
# o1-2024-12-17: $15/M input, $60/M output
# o3-2025-04-16: $2/M input, $8/M output
# o3-mini-2025-01-31: $1.1/M input, $4.4/M output - #TEXT ONLY
# gpt-4.1-2025-04-14: $2/M input, $8/M output
# gpt-4o-2024-11-20: $2.5/M input, $10/M output
# gpt-4-0613: $30/M input, $60/M output - #TEXT ONLY
# gpt-4-turbo-2024-04-09: $10/M input, $30/M output
# gpt-3.5-turbo-0125: $0.5/M input, $1.5/M output - #TEXT ONLY


def encode_image(image_path: str) -> str:
    """
    Translate image to base 64 for LLM interpretation

    :param image_path: string path to image
    :return: base64 encoded image
    """
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")


MessageContent = List[Dict[str, Union[str, Dict[str, str]]]]
ChatHistoryEntry = Dict[str, Union[str, MessageContent]]
ChatHistoryType = List[ChatHistoryEntry]


def call_llm(
    prompt: str,
    image_b64s: List[str],
    image_titles: List[str],
    chat_history: Optional[ChatHistoryType] = None,
    prompt_injections=None,
    prompt_index=None,
    max_retries: int = 10,
) -> Tuple[str, ChatHistoryType]:
    """
    Submit prompts, chat history, images to LLM. Get back LLMs response and updated
    chat history. I like the chat history approach for memory, its worked very well for
    me (some pruning and rehashing of the chat history to reduce tokens is advised for
    long workflows)

    :param prompt: string, prompt for LLM
    :param image_b64s: list of b64 strings
    :param image_titles: list of image titles (order must correspond with image_b64s)
    :param chat_history: List of prior user messages and llm responses. User messages
    and llm responses are dictionaries. Defaults to None.
    :param max_retries: how many retries if we hit some error
    :prompt_injections: optional injections from a feedback loop into a workflow
    :prompt_index: for feedback loop injection, need to know what index in the workflow we are on
    :return: LLM output (string), chat history now including the prompt / output generated by this cal
    """

    # feedback loop injections
    injection = None

    if prompt_injections:
        if type(prompt_injections) is not dict:
            print(f"Injection malformed for index {prompt_index}, integer attempt: {prompt_injections}")
            print(type(prompt_injections))
        else:
            try: # added as sometimes LLM is writing 'Prompt_1' in the rerun with injections dictionary
                injection = prompt_injections[str(prompt_index)]
            except:
                try:
                    injection = prompt_injections["prompt_" + str(prompt_index)]
                except:
                    try:
                        injection = prompt_injections["Prompt_" + str(prompt_index)]
                    except:
                        pass

    if chat_history is None:
        chat_history = []

    if llm.startswith("bedrock:"):
        model_id = llm[8:]
        if model_id.startswith("anthropic.claude") or model_id.startswith("eu.anthropic.claude") or model_id.startswith("us.anthropic.claude"):



            return _call_llm_bedrock_claude(
                model_id,
                prompt,
                image_b64s,
                image_titles,
                injection,
                chat_history,
                max_retries,
            )
        raise RuntimeError("Unsupported bedrock model: {}".format(model_id))

    raise RuntimeError("Unsupported model: {}".format(llm))

_CLIENTS = {}

def boto3_client(
    service_name: str,
    profile_name: str = os.environ["AWS_PROFILE"],
    region_name: str = os.environ["AWS_REGION"],
) -> boto3.client:
    """Get boto3 client for some service, picking up profile/region from env vars."""
    key = (service_name, profile_name, region_name)

    config = Config(
        read_timeout=300,  # seconds
        connect_timeout=30
    )


    global _CLIENTS
    if key not in _CLIENTS:
        session = boto3.Session(
            profile_name=profile_name,
        )
        logger.info(
            "boto3 session created: %s (%s)",
            session.profile_name,
            session.region_name,
        )

        client = session.client(
            service_name,
            region_name=region_name,
            config=config
        )
        logger.info(
            "boto3 client created: %s (%s)", service_name, client.meta.region_name
        )

        _CLIENTS[key] = client
    return _CLIENTS[key]

def _call_llm_bedrock_claude(
    model_id: str,
    prompt: str,
    image_b64s: List[str],
    image_titles: List[str],
    injection: Optional[str],
    chat_history: ChatHistoryType,
    max_retries: int,
) -> Tuple[str, ChatHistoryType]:

    start = time.time()

    print(f"Calling MODEL ID: {model_id}")

    if injection:  # Add the injection
        prompt += (
                "\n\n" + "PLEASE ALSO OBEY THESE FURTHER INSTRUCTIONS WHICH OVERWRITE ANY PRIOR INSTRUCTIONS THEY CONFLICT WITH" + "\n" + injection
        )

    # set up the current message in format for inference parameters using the Anthropic Claude Messages API
    # DOCS.AWS.AMAZON.COM/BEDROCK/LATEST/USERGUIDE/MODEL-PARAMETERS-ANTHROPIC-CLAUDE-MESSAGES.HTML

    current_message = [{"type": "text", "text": prompt}]
    for image_title, image_b64 in zip(image_titles, image_b64s):
        current_message.append({"type": "text", "text": f"{image_title}:"})
        current_message.append(
            {
                "type": "image",
                "source": {
                    "type": "base64",
                    "media_type": "image/png",
                    "data": image_b64,
                },
            }
        )

    # add the current message to chat history
    # all messages must have a role and content, so we add a message with the role 'user' and
    # content equal to current message

    chat_history.append({"role": "user", "content": current_message})

    max_tokens = 16000

    # to invoke models, we use bedrock runtime
    client = boto3_client(service_name="bedrock-runtime")

    # We must create a string (default) body for the invoke model method
    # Requirements can be found at:
    # DOCS.AWS.AMAZON.COM/BEDROCK/LATEST/USERGUIDE/MODEL-PARAMETERS-ANTHROPIC-CLAUDE-MESSAGES-REQUEST-RESPONSE.HTML

    body = json.dumps({"anthropic_version": "bedrock-2023-05-31",
                       "max_tokens": max_tokens,
                       "messages": chat_history,
                       "temperature": 0
    })

    # Make the request to Bedrock

    for attempt in range(max_retries):
        try:
            response = client.invoke_model(body=body, modelId=model_id)

            # Process the response, which contains a StreamingBody object which needs to be read
            # before parsing
            body = json.loads(response.get("body").read().decode("utf-8"))

            output = body["content"][0]["text"]

            # append LLM output to chat history
            chat_history.append({"role": "assistant", "content": output})
            llm_stats.update(response=body, model=llm)

            print(f"Finished call in {round(time.time() - start, 2)} seconds")

            return output, chat_history

        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            time.sleep(15)  # for rate limits

    raise RuntimeError("API failed after retries.")


def add_user_message_to_history(chat_history, message):

    current_message = [{"type": "text", "text": message}]

    chat_history.append({"role": "user", "content": current_message})

    return chat_history

def add_response_to_history(chat_history, response):

    chat_history.append({"role": "assistant", "content": response})

    return chat_history

def main():

    llm_stats.reset()

    answer_1, chat_history = call_llm(
        "What is Christmas?", [], [], chat_history=None, max_retries=10
    )

    print(answer_1)
    print(llm_stats.get_stats())

    llm_log(chat_history, "demo_call")



if __name__ == "__main__":

    main()
